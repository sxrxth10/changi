{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8de9991",
   "metadata": {},
   "source": [
    "### Check a random url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "08d1a069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random URL https://www.jewelchangiairport.com/en/tourist-perks-and-promotions.html was FOUND in the scraped data.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "import os\n",
    "\n",
    "# List of URLs to check\n",
    "check_urls = [\n",
    "    \"https://www.jewelchangiairport.com/en/tourist-perks-and-promotions.html\",\n",
    "    # Add more URLs as needed\n",
    "]\n",
    "\n",
    "# Path to scraped data\n",
    "data_file = \"scraped_data/changi_english_data.json\"\n",
    "\n",
    "# Read scraped data\n",
    "if not os.path.exists(data_file):\n",
    "    print(f\"Error: {data_file} not found. Run the Scrapy spider first.\")\n",
    "    exit(1)\n",
    "\n",
    "with open(data_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    scraped_data = json.load(f)\n",
    "\n",
    "# Get URLs from scraped data\n",
    "scraped_urls = [page[\"url\"] for page in scraped_data]\n",
    "\n",
    "# Randomly select a URL to check\n",
    "random_url = random.choice(check_urls)\n",
    "\n",
    "# Check if random URL is in scraped data\n",
    "if random_url in scraped_urls:\n",
    "    print(f\"Random URL {random_url} was FOUND in the scraped data.\")\n",
    "else:\n",
    "    print(f\"Random URL {random_url} was NOT found in the scraped data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4393c8b6",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293c3b0d",
   "metadata": {},
   "source": [
    "### Filter out non English text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fae996d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English content saved to scraped_data/changi_english_data.json (588 pages)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from langdetect import detect, DetectorFactory\n",
    "import langdetect\n",
    "\n",
    "# Ensure consistent language detection\n",
    "DetectorFactory.seed = 0\n",
    "\n",
    "# Input and output files\n",
    "input_file = \"scraped_data/changi_all_data.json\"\n",
    "output_file = \"scraped_data/changi_english_data.json\"\n",
    "\n",
    "# Check if input file exists\n",
    "if not os.path.exists(input_file):\n",
    "    print(f\"Error: {input_file} not found. Run the Scrapy spider first.\")\n",
    "    exit(1)\n",
    "\n",
    "# Read scraped data\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    scraped_data = json.load(f)\n",
    "\n",
    "# Filter English content\n",
    "english_data = []\n",
    "for page in scraped_data:\n",
    "    url = page[\"url\"]\n",
    "    content = page[\"content\"]\n",
    "    english_content = []\n",
    "    \n",
    "    for text in content:\n",
    "        try:\n",
    "            # Detect language; keep only English text\n",
    "            if detect(text) == \"en\":\n",
    "                english_content.append(text)\n",
    "        except langdetect.lang_detect_exception.LangDetectException:\n",
    "            # Skip text too short or undetectable\n",
    "            continue\n",
    "    \n",
    "    if english_content:  # Only include pages with English content\n",
    "        english_data.append({\"url\": url, \"content\": english_content})\n",
    "\n",
    "# Save English-only data\n",
    "os.makedirs(\"scraped_data\", exist_ok=True)\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(english_data, f, indent=2)\n",
    "print(f\"English content saved to {output_file} ({len(english_data)} pages)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6975a2ba",
   "metadata": {},
   "source": [
    "### Considering numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d3e5c20b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English and numeric content saved to scraped_data/changi_english_and_numbers_data.json (594 pages)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "from langdetect import detect, DetectorFactory\n",
    "import langdetect\n",
    "\n",
    "# Ensure consistent language detection\n",
    "DetectorFactory.seed = 0\n",
    "\n",
    "# Input and output files\n",
    "input_file = \"scraped_data/changi_all_data.json\"\n",
    "output_file = \"scraped_data/changi_english_and_numbers_data.json\"\n",
    "\n",
    "# Check if input file exists\n",
    "if not os.path.exists(input_file):\n",
    "    print(f\"Error: {input_file} not found. Run the Scrapy spider first.\")\n",
    "    exit(1)\n",
    "\n",
    "# Read scraped data\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    scraped_data = json.load(f)\n",
    "\n",
    "# Function to check if text is primarily numeric\n",
    "def is_numeric(text):\n",
    "    # Remove whitespace and check if text contains mostly numbers\n",
    "    cleaned_text = text.strip()\n",
    "    # Matches numbers, decimals, or numbers with common symbols (e.g., $12.99, 12:30)\n",
    "    return bool(re.match(r'^[\\d\\s.,:;$-]+$', cleaned_text))\n",
    "\n",
    "# Filter English and numeric content\n",
    "english_data = []\n",
    "for page in scraped_data:\n",
    "    url = page[\"url\"]\n",
    "    content = page[\"content\"]\n",
    "    filtered_content = []\n",
    "    \n",
    "    for text in content:\n",
    "        try:\n",
    "            # Keep numeric text or English text\n",
    "            if is_numeric(text) or detect(text) == \"en\":\n",
    "                filtered_content.append(text)\n",
    "        except langdetect.lang_detect_exception.LangDetectException:\n",
    "            # Keep numeric text even if language detection fails\n",
    "            if is_numeric(text):\n",
    "                filtered_content.append(text)\n",
    "    \n",
    "    if filtered_content:  # Only include pages with filtered content\n",
    "        english_data.append({\"url\": url, \"content\": filtered_content})\n",
    "\n",
    "# Save filtered data\n",
    "os.makedirs(\"scraped_data\", exist_ok=True)\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(english_data, f, indent=2)\n",
    "print(f\"English and numeric content saved to {output_file} ({len(english_data)} pages)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb070883",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ec2a261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned data saved to scraped_data/changi_cleaned_data.json (579 pages)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "\n",
    "# Input and output files\n",
    "input_file = \"scraped_data/changi_english_data_2.json\"\n",
    "output_file = \"scraped_data/changi_cleaned_data.json\"\n",
    "\n",
    "# Check if input file exists\n",
    "if not os.path.exists(input_file):\n",
    "    print(f\"Error: {input_file} not found. Run the filtering script first.\")\n",
    "    exit(1)\n",
    "\n",
    "# Read scraped data\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Function to clean and normalize text\n",
    "def clean_text(text):\n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text.strip())\n",
    "    # Remove special characters, keep alphanumeric, numbers, and common punctuation\n",
    "    text = re.sub(r'[^\\w\\s.,:;$-]', '', text)\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "# Clean data\n",
    "cleaned_data = []\n",
    "for page in data:\n",
    "    url = page[\"url\"]\n",
    "    content = page[\"content\"]\n",
    "    cleaned_content = []\n",
    "    \n",
    "    for text in content:\n",
    "        # Clean and normalize text\n",
    "        cleaned = clean_text(text)\n",
    "        # Filter out short or irrelevant text (e.g., < 3 characters or single words)\n",
    "        if len(cleaned) >= 3 and len(cleaned.split()) > 1:\n",
    "            cleaned_content.append(cleaned)\n",
    "    \n",
    "    if cleaned_content:  # Only include pages with cleaned content\n",
    "        cleaned_data.append({\"url\": url, \"content\": cleaned_content})\n",
    "\n",
    "# Save cleaned data\n",
    "os.makedirs(\"scraped_data\", exist_ok=True)\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(cleaned_data, f, indent=2)\n",
    "print(f\"Cleaned data saved to {output_file} ({len(cleaned_data)} pages)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1c2aad",
   "metadata": {},
   "source": [
    "### Chunking and Adding metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5f387fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pinecone\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Pinecone\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "27db57c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "PINECONE_API_KEY = os.getenv(\"your-pinecone-api-key\")\n",
    "PINECONE_ENV = \"us-west1-gcp\"\n",
    "INDEX_NAME = \"changiindex\"  # change as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "316b6e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"scraped_data/changi_cleaned_data.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine content into one string per document\n",
    "documents = []\n",
    "for entry in raw_data:\n",
    "    url = entry[\"url\"]\n",
    "    content = entry.get(\"content\", [])\n",
    "    text = \" \".join(content).strip()\n",
    "    if text:\n",
    "        documents.append(Document(page_content=text, metadata={\"source\": url}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "98419255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded and split 3324 chunks.\n"
     ]
    }
   ],
   "source": [
    "# ‚úÇÔ∏è Chunking the Documents\n",
    "# ---------------------\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=800,\n",
    "    chunk_overlap=200,\n",
    ")\n",
    "split_docs = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"‚úÖ Loaded and split {len(split_docs)} chunks.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "39ce0b51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sarat\\AppData\\Local\\Temp\\ipykernel_17956\\1513018633.py:6: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_model = HuggingFaceEmbeddings(model_name=model_name)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec163b86cd8e4fed855effa7f597bd09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sarat\\AppData\\Roaming\\Python\\Python312\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\sarat\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "839bb972ecc147fe94b6492360f35e03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2696723de51740a3bd510836976453eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1aa2b84f120348a683020046d3ea308c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8a4d26057bb470688e40b47a3c28187",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efe9d57f210c4e158f00b250166d8344",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92c4f23fd55e48caa431571a68cb8b1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0590c94622cd4afdb278553d797fb9e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "949a41a9a0e7482e95524d0f8d883255",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5e95400b4ed4f1ca1173f7c2d62c811",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47fd8c8f960244f08ec9fb87dd0842da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# Load MiniLM\n",
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "98977ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "index = pc.Index(\"changi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "860331a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Successfully indexed using Pinecone v3 + langchain-pinecone.\n"
     ]
    }
   ],
   "source": [
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "\n",
    "vectorstore = PineconeVectorStore.from_documents(\n",
    "    documents=split_docs,\n",
    "    embedding=embedding_model,\n",
    "    index_name=INDEX_NAME,\n",
    "    pinecone_api_key=PINECONE_API_KEY,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Successfully indexed using Pinecone v3 + langchain-pinecone.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fb700571",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Index.describe_index_stats of <pinecone.db_data.index.Index object at 0x0000016DA2DBA8A0>>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.describe_index_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "409676fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sarat\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(id='12cd8641-2e1b-4af5-8af9-e8dd2279d39c', metadata={'source': 'https://www.changiairport.com/en/at-changi/facilities-and-services-directory/passenger-meeting-services.html'}, page_content='and departures, ensuring a memorable beginning and end to your journey in singapore. upon arrival, passengers can expect a personalised greeting at the gate, luggage and immigration assistance for a hassle-free process, and coordination of onward transportation as needed. for departures, our services include guidance through check-in and security, and escort to the departure gate. these passenger meeting services are designed to cater to the needs of all travellers, whether youre visiting for business or leisure, ensuring a smooth and enjoyable experience at changi airport. to request our meet and greet services, passengers or their representatives can easily make a booking through our official changi airport website or the changi airport mobile app. when booking, please fill in the'), Document(id='7aa6558a-f022-4d84-aed6-dcc31af910ce', metadata={'source': 'https://www.changiairport.com/en/experience/attractions-directory.html?category=play'}, page_content='enjoy events fit for your entire family. an exciting collection for all art enthusiasts. be immersed in the natural wonders within changi. redeem perks and purchase attraction tickets easily at any time. follow us sign up for a changi account to receive the latest updates  2025 changi airport lets give you the best experience possible changi airport uses cookies and other innovative tech to deliver an incredible and more personalized experience. analytical technologies give us insights on site usage to improve our services. marketing technologies from changiairport.com and trusted partners help us advertise our services more relevantly. if you choose to'), Document(id='7f1c9e2a-0b9d-4b39-87f0-310564dd7e90', metadata={'source': 'https://www.changiairport.com/en/experience/attractions-directory.html?location=t4'}, page_content='enjoy events fit for your entire family. an exciting collection for all art enthusiasts. be immersed in the natural wonders within changi. redeem perks and purchase attraction tickets easily at any time. follow us sign up for a changi account to receive the latest updates  2025 changi airport lets give you the best experience possible changi airport uses cookies and other innovative tech to deliver an incredible and more personalized experience. analytical technologies give us insights on site usage to improve our services. marketing technologies from changiairport.com and trusted partners help us advertise our services more relevantly. if you choose to')]\n"
     ]
    }
   ],
   "source": [
    "# Sample query\n",
    "sample_vector = embedding_model.embed_query(\"changai airport holiday plans\")\n",
    "result = vectorstore.similarity_search(query=\"changai airport holiday plans\",k=3)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8819d515",
   "metadata": {},
   "source": [
    "### Rag pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "bfa10df8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sarat\\AppData\\Local\\Temp\\ipykernel_17956\\1998517984.py:45: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result = rag_chain(query)\n",
      "C:\\Users\\sarat\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ü§ñ Bot: I'm sorry, I don't have enough information to understand your question. Can you please provide more context or clarify what you would like to know?\n",
      "\n",
      "üìÑ Sources:\n",
      "- https://www.changiairport.com/zh.html\n",
      "- https://www.changiairport.com/zh/cookie-policy.html\n",
      "- https://www.changiairport.com/en/corporate/partnering-us/advertising-and-sponsorship.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sarat\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ü§ñ Bot: I don't have information on specific holiday plans offered by Changi Airport.\n",
      "\n",
      "üìÑ Sources:\n",
      "- https://www.changiairport.com/zh/help/changi-app/changi-pay/overseas-payment.html\n",
      "- https://www.changiairport.com/en/corporate/partnering-us/airport-concessions/airport-retail.html\n",
      "- https://www.changiairport.com/en/fly/transit-guide.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sarat\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ü§ñ Bot: Changi Airport and Jewel offer a wide range of dining options, including home-grown eateries and international cuisines. You can easily make reservations through the Changi app for a comfortable dining experience. Additionally, there are deals available for discounts of up to 50% at selected restaurants. You can explore the dining options by scanning the QR code on the Changi app or by signing up for a Changi account to receive the latest updates.\n",
      "\n",
      "üìÑ Sources:\n",
      "- https://www.changiairport.com/en/help/changi-app/dine.html\n",
      "- https://www.changiairport.com/en/fly/transit-guide.html\n",
      "- https://www.jewelchangiairport.com/en/venue-hire.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sarat\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ü§ñ Bot: Changi Airport offers a variety of entertainment options, including free movies, live performances, sports events, and more. Additionally, visitors can enjoy interactive games, immersive shows, and amazing projection experiences in the virtual world of Changi Airport. There are also 20 different interactive touchpoints with various content to explore and enjoy.\n",
      "\n",
      "üìÑ Sources:\n",
      "- https://www.changiairport.com/en/happenings/events-directory/sg60-outdoor-light-up.html?category=play-attractions\n",
      "- https://www.changiairport.com/en/happenings/events-directory/sg60-outdoor-light-up.html\n",
      "- https://www.jewelchangiairport.com/en/attractions/ces.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sarat\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ü§ñ Bot: I'm sorry, I don't have enough context to understand your question. How can I assist you today?\n",
      "\n",
      "üìÑ Sources:\n",
      "- https://www.changiairport.com/zh.html\n",
      "- https://www.changiairport.com/zh/cookie-policy.html\n",
      "- https://www.changiairport.com/en/corporate/partnering-us/advertising-and-sponsorship.html\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "\n",
    "# Init embedding and vectorstore\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "vectorstore = PineconeVectorStore.from_existing_index(\n",
    "    index_name=\"changiindex\",\n",
    "    embedding=embedding_model,\n",
    "    \n",
    ")\n",
    "\n",
    "# Create the retriever\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "# Init OpenAI Chat Model\n",
    "llm = ChatOpenAI(\n",
    "    openai_api_key=OPENAI_API_KEY,\n",
    "    model_name=\"gpt-3.5-turbo\",  # or \"gpt-4\"\n",
    "    temperature=0.2\n",
    ")\n",
    "\n",
    "# Build RetrievalQA chain (RAG pipeline)\n",
    "rag_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "# Run chatbot loop\n",
    "while True:\n",
    "    query = input(\"\\nüí¨ You: \")\n",
    "    if query.lower() in [\"exit\", \"quit\"]:\n",
    "        break\n",
    "\n",
    "    result = rag_chain(query)\n",
    "    print(f\"\\nü§ñ Bot: {result['result']}\")\n",
    "    \n",
    "    # (Optional) Show sources\n",
    "    print(\"\\nüìÑ Sources:\")\n",
    "    for doc in result[\"source_documents\"]:\n",
    "        print(\"-\", doc.metadata[\"source\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2b0ff0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1eca66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
